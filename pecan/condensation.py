"""Topology-based diffusion condensation scheme."""

import argparse
import datetime
import logging
import os
import sys

import numpy as np

from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics.pairwise import rbf_kernel

from yaspin import yaspin
from yaspin.spinners import Spinners

from callbacks import CalculateDiffusionHomology
from callbacks import CalculatePersistentHomology

import data

from utilities import estimate_epsilon
from utilities import generate_output_filename


class DiffusionCondensation:
    """Generic diffusion condensation functor.

    This class permits running a generic diffusion condensation process
    on a data set. It supports certain hooks by which additional steps,
    such as further processing operations, may be integrated.
    """

    def __init__(
        self,
        callbacks=[],
        prefix='data_',
    ):
        """Initialise new instance and register callbacks.

        Parameters
        ----------
        callbacks : list of callable
            Function objects (functors) that will be called during each
            iteration. Every functor instance must satisfy a signature,
            as described below::

                callback(i, X, P, D)

            Where `i` is the current time step, `X` is the current data,
            `P` is the current diffusion operator, and `D` is a distance
            matrix between data points (using the Euclidean distance).

        prefix : str
            Indicates the prefix to be used for storing individual time
            steps. If set to `X`, the first key of the diffusion
            condensation process will be called `X_t_0`.
        """
        self.callbacks = callbacks
        self.prefix = prefix

        # TODO: this could be made configurable, but at present, I am
        # relying on a precise signature of this function.
        self.kernel_fn = self.make_affinity_matrix

    def __call__(self, X, epsilon):
        """Run condensation process for a given data set."""
        n = X.shape[0]

        # Denotes the previous density measurement, which is initialised to
        # an identity matrix depending on the number of samples, as well as
        # the previously-observed difference, which has to be set to inf in
        # order to be suitable.
        Q_prev = np.identity(n)
        Q_diff = np.inf

        # We only work on a copy of the data set since we shift everything,
        # i.e. points start changing their positions.
        X = X.copy()

        i = 0
        j = -2

        # Will store the data set per iteration to check whether the
        # implementation works as expected.
        data = {
            self.prefix + 't_0': X.copy(),
            'P_t_0': np.identity(n),
        }

        for callback in self.callbacks:
            callback(i, X, np.identity(n), euclidean_distances(X))

        logging.info('Started diffusion condensation process')

        with yaspin(spinner=Spinners.dots) as sp:
            while i - j > 1:

                j = i

                while Q_diff >= 1e-4:

                    sp.text = f'Iteration {i}'

                    # This signals that we want to perform an additional
                    # operation of diffusion here.
                    i += 1

                    # Process new merges by checking whether their respective
                    # label assignments changed.
                    D = euclidean_distances(X)

                    A = self.kernel_fn(X, epsilon)
                    Q = np.sum(A, axis=1)
                    K = np.diag(1.0 / Q) @ A @ np.diag(1.0 / Q)
                    P = np.diag(1.0 / np.sum(K, axis=1)) @ K

                    # Store diffusion operator
                    data[f'P_t_{i}'] = P

                    for callback in self.callbacks:
                        callback(i, X, P, D)

                    X = P @ X

                    # Store new variant of the data set for the current
                    # iteration at time $i$.
                    data[f'{self.prefix}t_{i}'] = X.copy()

                    Q_diff = np.max(Q - Q_prev)
                    Q_prev = Q

                epsilon *= 2
                Q_diff = np.inf

        # Update data dictionary with all the data generated by the
        # callback.
        for callback in self.callbacks:
            data = callback.finalise(data)

        logging.info('Finished diffusion condensation process')
        return data

    @staticmethod
    def make_affinity_matrix(X, epsilon):
        """Calculate affinity matrix.

        This functions calculates an affinity matrix from an input matrix.
        The input matrix is required to be of shape $(n, d)$, with $n$ and
        $d$ representing the number of samples and dimensions.

        Parameters
        ----------
        X : np.array of shape (n, m)
            Input matrix with `n` samples and `m` features.

        epsilon : float
            Smoothing parameter for the kernel calculation.
        """
        return rbf_kernel(X, gamma=1.0 / epsilon)


if __name__ == '__main__':

    # Set up logging to obtain some nice output information, runtime,
    # and much more.
    logging.basicConfig(
        format='%(asctime)s.%(msecs)03d  [%(levelname)-10s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        level=logging.INFO
    )

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-d', '--data',
        default='hyperuniform_ellipse',
        type=str,
    )

    parser.add_argument(
        '-n', '--num-samples',
        default=128,
        type=int
    )

    parser.add_argument(
        '-e', '--epsilon',
        # TODO: ensure that this makes sense and be adjusted more
        # easily, depending on the number of points etc.
        default=np.nan,
        type=float,
    )

    parser.add_argument(
        '-o', '--output',
        default='.',
        type=str,
        help='Output directory'
    )

    # TODO: implement effects of this
    parser.add_argument(
        '--noise',
        type=float,
        default=0.0,
        help='Noise level to add to the data set'
    )

    parser.add_argument(
        '-r',
        default=0.5,
        type=float,
        help='Inner radius for annuli and related data sets. Will be used '
             'whenever it is appropriate.'
    )

    parser.add_argument(
        '-R',
        default=1.0,
        type=float,
        help='Outer radius for annuli and related data sets. Will be used '
             'whenever it is appropriate.'
    )

    args = parser.parse_args()
    this = sys.modules[__name__]

    # Search for a generator routine, as requested by the client. This
    # does not fail gracefully.
    generator = getattr(data, args.data)

    logging.info(f'Using generator routine {generator}')

    # Not the best way to seed the random generator, but this ensures
    # that we obtain different results per run.
    seed = int(datetime.datetime.now().timestamp())

    X, C = generator(
        args.num_samples,
        random_state=seed,
        r=args.r,
        R=args.R
    )

    if np.isnan(args.epsilon):
        args.epsilon = estimate_epsilon(X)

        logging.info(
            f'Epsilon parameter has not been set. Estimating '
            f'it as {args.epsilon:.4f}.'
        )

    logging.info(f'Data set: {args.data}')
    logging.info(f'Number of samples: {args.num_samples}')
    logging.info(f'Epsilon: {args.epsilon:.4f}')

    callbacks = [
        CalculateDiffusionHomology(),
        CalculatePersistentHomology()
    ]

    diffusion_condensation = DiffusionCondensation(callbacks=callbacks)
    data = diffusion_condensation(X, args.epsilon)

    # Store data set. The name of output file is generated automatically
    # to account for conditions of the environment.

    output_filename = generate_output_filename(args, seed)
    output_filename = os.path.join(args.output, output_filename)

    logging.info(f'Storing results in {output_filename}')

    np.savez(output_filename, **data)
